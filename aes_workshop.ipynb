{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import os\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a few paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path where fake audio files are saved\n",
    "# data_path = '/data/Famous_Figures/AES_Data/aes_data'\n",
    "data_path = '/data/Famous_Figures/AES_Data/aes_data_laundered'\n",
    "\n",
    "# path where features will be saved\n",
    "# feat_dir = '/data/Famous_Figures/AES_Features/'\n",
    "feat_dir = '/data/Famous_Figures/AES_Features_laundered/'\n",
    "\n",
    "# path where score files will be saved\n",
    "# score_dir = '/data/Famous_Figures/AES_Score_Files/'\n",
    "score_dir = '/data/Famous_Figures/AES_Score_Files_laundered/'\n",
    "\n",
    "if not os.path.exists(score_dir):\n",
    "    os.makedirs(score_dir)\n",
    "\n",
    "# extension of the audio files\n",
    "audio_ext = '.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CQCC-GMM and LFCC-GMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ASD_ML.gmm_asvspoof import scoring\n",
    "\n",
    "features = 'cqcc'\n",
    "model_dir = 'ASD_ML/gmm_' + str(512) + '_LA_' + features\n",
    "bona_path = os.path.join(model_dir, 'bonafide', 'gmm_final.pkl')\n",
    "spoof_path = os.path.join(model_dir, 'spoof', 'gmm_final.pkl')\n",
    "\n",
    "dict_file = dict()\n",
    "dict_file['bona'] = bona_path\n",
    "dict_file['spoof'] = spoof_path\n",
    "\n",
    "# files = ['Barack_Obama_StyleTTS2', 'Trump_Parrot_1', 'Trump_Parrot_2', 'Joe_Biden_ElevenLabs_1', 'Joe_Biden_ElevenLabs_2']\n",
    "files = os.listdir(data_path)\n",
    "files = [f.split('.')[0] for f in files]\n",
    "\n",
    "eval_folder = data_path\n",
    "\n",
    "scores_file = os.path.join(score_dir, 'scores-' + features + '-gmm-' + str(512) + '.txt')\n",
    "\n",
    "test_scores = scoring(scores_file=scores_file, dict_file=dict_file, features=features,\n",
    "        eval_file_list=files, eval_folder=eval_folder, audio_ext=audio_ext,\n",
    "        feat_dir=feat_dir, features_cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OC-Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## imports #############\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"./AIR-ASVspoof/\")\n",
    "from process_LA_data import extract_lfcc\n",
    "\n",
    "############## Paths and Variables ###############\n",
    "\n",
    "# filelist = ['Barack_Obama_StyleTTS2', 'Trump_Parrot_1', 'Trump_Parrot_2', 'Joe_Biden_ElevenLabs_1', 'Joe_Biden_ElevenLabs_2']\n",
    "# labels = [\"spoof\", \"spoof\", \"spoof\", \"spoof\", \"spoof\"]\n",
    "\n",
    "files = os.listdir(data_path)\n",
    "filelist = [f.split('.')[0] for f in files]\n",
    "labels = [\"spoof\" for _ in range(len(filelist))]\n",
    "\n",
    "model_dir = \"./AIR-ASVspoof/models/ocsoftmax\"\n",
    "model_path = os.path.join(model_dir, \"anti-spoofing_lfcc_model.pt\")\n",
    "loss_model_path = os.path.join(model_dir, \"anti-spoofing_loss_model.pt\")\n",
    "\n",
    "add_loss =  \"ocsoftmax\"\n",
    "\n",
    "Feat_dir = os.path.join(feat_dir, 'lfcc_features_airasvspoof')\n",
    "LFCC_sav_dir = os.path.join(Feat_dir, 'eval')\n",
    "audio_ext = '.wav'\n",
    "\n",
    "if not os.path.exists(LFCC_sav_dir):\n",
    "    os.makedirs(LFCC_sav_dir)\n",
    "\n",
    "#################### Extract Features ######################\n",
    "for file in filelist:\n",
    "\n",
    "    LFCC_filename = os.path.join(LFCC_sav_dir, str(file) + '.pkl')\n",
    "\n",
    "    if not os.path.exists(LFCC_filename):\n",
    "\n",
    "        # audio_file = os.path.join(pathToDatabase, 'ASVspoof2019_' + access_type + '_eval/flac', file + '.flac')\n",
    "        audio_file = os.path.join(data_path, str(file) + audio_ext)\n",
    "\n",
    "        x, fs = librosa.load(audio_file)\n",
    "        \n",
    "        lfcc_featues = extract_lfcc(x, fs)\n",
    "\n",
    "        print(lfcc_featues.shape)\n",
    "\n",
    "        with open(LFCC_filename, 'wb') as f:\n",
    "            pickle.dump(lfcc_featues, f)\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"Feature file {} already extracted\".format(file))\n",
    "\n",
    "\n",
    "#################### Generate Scores ######################\n",
    "\n",
    "def repeat_padding(spec, ref_len):\n",
    "    mul = int(np.ceil(ref_len / spec.shape[1]))\n",
    "    spec = spec.repeat(1, mul)[:, :ref_len]\n",
    "    return spec\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torch.load(model_path, map_location=\"cuda\")\n",
    "model = model.to(device)\n",
    "loss_model = torch.load(loss_model_path) if add_loss != \"softmax\" else None\n",
    "\n",
    "model.eval()\n",
    "\n",
    "scr = []\n",
    "\n",
    "with open(os.path.join(score_dir, 'AES_Workshop_checkpoint_cm_score.txt'), 'w') as cm_score_file:\n",
    "\n",
    "    for i, audio_fn in enumerate(tqdm(filelist)):\n",
    "\n",
    "        LFCC_filename = os.path.join(LFCC_sav_dir, str(audio_fn) + '.pkl')\n",
    "        \n",
    "        with open(LFCC_filename, 'rb') as feature_handle:\n",
    "            feat_mat = pickle.load(feature_handle)\n",
    "\n",
    "        feat_mat = torch.from_numpy(feat_mat)\n",
    "        feat_len = 750\n",
    "        this_feat_len = feat_mat.shape[1]\n",
    "        if this_feat_len > feat_len:\n",
    "            startp = np.random.randint(this_feat_len-feat_len)\n",
    "            feat_mat = feat_mat[:, startp:startp+feat_len]\n",
    "        if this_feat_len < feat_len:\n",
    "            \n",
    "            feat_mat = repeat_padding(feat_mat, feat_len)\n",
    "\n",
    "        print(feat_mat.shape)\n",
    "\n",
    "        # lfcc_feat = feat_mat.unsqueeze(1).float()\n",
    "        # print(lfcc_feat.shape)\n",
    "        lfcc_feat = feat_mat.unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "        print(lfcc_feat.shape)\n",
    "        \n",
    "        label = labels[i]\n",
    "\n",
    "        feats, lfcc_outputs = model(lfcc_feat)\n",
    "\n",
    "        score = F.softmax(lfcc_outputs)[:, 0]\n",
    "\n",
    "        if add_loss == \"ocsoftmax\":\n",
    "            ang_isoloss, score = loss_model(feats, labels)\n",
    "        elif add_loss == \"amsoftmax\":\n",
    "            outputs, moutputs = loss_model(feats, labels)\n",
    "            score = F.softmax(outputs, dim=1)[:, 0]\n",
    "\n",
    "        \n",
    "        cm_score_file.write(\n",
    "            '%s %s %s\\n' % (audio_fn, label, score.item()))\n",
    "    \n",
    "        scr.append(score.item())\n",
    "\n",
    "scores_df_ocsoftmax = pd.DataFrame({'files': filelist, 'scores': scr})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_df_ocsoftmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RawNet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## imports ##########\n",
    "import yaml\n",
    "import librosa\n",
    "from torch import Tensor\n",
    "\n",
    "sys.path.append(\"./RawNet2/\")\n",
    "from model import RawNet\n",
    "\n",
    "############# Paths and Variables #############\n",
    "\n",
    "# file_eval = ['Barack_Obama_StyleTTS2', 'Trump_Parrot_1', 'Trump_Parrot_2', 'Joe_Biden_ElevenLabs_1', 'Joe_Biden_ElevenLabs_2']\n",
    "# labels = [\"spoof\", \"spoof\", \"spoof\", \"spoof\", \"spoof\"]\n",
    "\n",
    "files = os.listdir(data_path)\n",
    "file_eval = [f.split('.')[0] for f in files]\n",
    "labels = [\"spoof\" for _ in range(len(filelist))]\n",
    "\n",
    "eval_out = os.path.join(score_dir, 'RawNet2_' + '_eval_CM_scores.txt')\n",
    "model_path = './RawNet2/models/pre_trained_DF_RawNet2.pth'\n",
    "\n",
    "############# Black Box Code ############\n",
    "\n",
    "dir_yaml = os.path.splitext('./RawNet2/model_config_RawNet')[0] + '.yaml'\n",
    "\n",
    "with open(dir_yaml, 'r') as f_yaml:\n",
    "    parser1 = yaml.load(f_yaml, yaml.Loader)\n",
    "\n",
    "\n",
    "track = 'LA'\n",
    "assert track in ['LA', 'PA','DF'], 'Invalid track given'\n",
    "\n",
    "#GPU device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'                  \n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "#model \n",
    "model = RawNet(parser1['model'], device)\n",
    "nb_params = sum([param.view(-1).size()[0] for param in model.parameters()])\n",
    "model =(model).to(device)\n",
    "\n",
    "#set Adam optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=args.lr,weight_decay=args.weight_decay)\n",
    "\n",
    "if model_path:\n",
    "    model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "    print('Model loaded : {}'.format(model_path))\n",
    "\n",
    "print('no. of eval trials',len(file_eval))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# functions we may need\n",
    "def pad(x, max_len=64600):\n",
    "    x_len = x.shape[0]\n",
    "    if x_len >= max_len:\n",
    "        return x[:max_len]\n",
    "    # need to pad\n",
    "    num_repeats = int(max_len / x_len)+1\n",
    "    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]\n",
    "    \n",
    "    return padded_x\t\n",
    "\n",
    "score_list = []  \n",
    "\n",
    "for utt_id, audio_fn in enumerate(file_eval):\n",
    "\n",
    "    X, fs = librosa.load(os.path.join(data_path, str(audio_fn) + audio_ext), sr=16000)\n",
    "    \n",
    "    X_pad = pad(X, 64600)\n",
    "    x_inp = Tensor(X_pad)\n",
    "\n",
    "    x_inp = x_inp.unsqueeze(0).float().to(device)\n",
    "    \n",
    "    score_out = model(x_inp)\n",
    "\n",
    "    score_out = (score_out[:, 1]).data.cpu().numpy().ravel()\n",
    "\n",
    "    # add outputs\n",
    "    score_list.extend(score_out.tolist())\n",
    "\n",
    "    print(score_list)\n",
    "    \n",
    "with open(eval_out, 'a+') as fh:\n",
    "    for f, cm in zip(file_eval,score_list):\n",
    "        fh.write('{} {}\\n'.format(f, cm))\n",
    "fh.close()   \n",
    "print('Scores saved to {}'.format(eval_out))\n",
    "\n",
    "scores_df_rawnet2 = pd.DataFrame({'files': file_eval, 'scores': score_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_df_rawnet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "score_file_path = '/data/Famous_Figures/AES_Score_Files/'\n",
    "score_file_path_laundered = '/data/Famous_Figures/AES_Score_Files_laundered/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Score files of CQCC-GMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'scores-cqcc-gmm-512.txt'\n",
    "\n",
    "scores_fulfile = os.path.join(score_file_path, filename)\n",
    "scores_fulfile_laundered = os.path.join(score_file_path_laundered, filename)\n",
    "\n",
    "scores_df = pd.read_csv(scores_fulfile, sep=\" \", names=[\"AUDIO_FILE_NAME\", \"Scores\"])\n",
    "\n",
    "scores_df_laundered = pd.read_csv(scores_fulfile_laundered, sep=\" \", names=[\"AUDIO_FILE_NAME\", \"Scores\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_styler = scores_df.style.set_table_attributes(\"style='display:inline'\").set_caption('Before Laundering')\n",
    "df2_styler = scores_df_laundered.style.set_table_attributes(\"style='display:inline'\").set_caption('After Laundering')\n",
    "\n",
    "display_html(df1_styler._repr_html_()+df2_styler._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Score files of OC-Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'AES_Workshop_checkpoint_cm_score.txt'\n",
    "\n",
    "scores_fulfile = os.path.join(score_file_path, filename)\n",
    "scores_fulfile_laundered = os.path.join(score_file_path_laundered, filename)\n",
    "\n",
    "scores_df = pd.read_csv(scores_fulfile, sep=\" \", names=[\"AUDIO_FILE_NAME\", \"Key\", \"Scores\"])\n",
    "\n",
    "scores_df_laundered = pd.read_csv(scores_fulfile_laundered, sep=\" \", names=[\"AUDIO_FILE_NAME\", \"Key\", \"Scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_styler = scores_df.style.set_table_attributes(\"style='display:inline'\").set_caption('Before Laundering')\n",
    "df2_styler = scores_df_laundered.style.set_table_attributes(\"style='display:inline'\").set_caption('After Laundering')\n",
    "\n",
    "display_html(df1_styler._repr_html_()+df2_styler._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Score files of RawNet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'RawNet2__eval_CM_scores.txt'\n",
    "\n",
    "scores_fulfile = os.path.join(score_file_path, filename)\n",
    "scores_fulfile_laundered = os.path.join(score_file_path_laundered, filename)\n",
    "\n",
    "scores_df = pd.read_csv(scores_fulfile, sep=\" \", names=[\"AUDIO_FILE_NAME\", \"Scores\"])\n",
    "\n",
    "scores_df_laundered = pd.read_csv(scores_fulfile_laundered, sep=\" \", names=[\"AUDIO_FILE_NAME\", \"Scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_styler = scores_df.style.set_table_attributes(\"style='display:inline'\").set_caption('Before Laundering')\n",
    "df2_styler = scores_df_laundered.style.set_table_attributes(\"style='display:inline'\").set_caption('After Laundering')\n",
    "\n",
    "display_html(df1_styler._repr_html_()+df2_styler._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
